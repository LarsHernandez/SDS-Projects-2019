---
title: "M3 Assignment 1 - Fake Trump"
author: "Lars Nielsen"
date: "18/11/2019"
output:
  html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---
# Introduction
Deadline: 18/11 - 2019

The site [https://faketrump.ai/](https://faketrump.ai/) is an interesting example of AI-powered fake-text generation. They write:

>We built an artificial intelligence model by fine-tuning [GPT-2](https://openai.com/blog/better-language-models/) to generate tweets in the style of Donald Trump’s Twitter account. After seeing the results, we also built a discriminator that can accurately detect fake tweets 77% of the time — think you can beat our classifier? Try it yourself!

GPT-2 is a neural transformer-based model, that has been announced by OpenAI in February 2019 and created considerable discussion because they decided - in contrast to their earlier policies - not to release the mode to the public. Their central argument was that the model could be used to produce fake news, spam and alike too easily. The footnote of the faketrump page reads: “Generating realistic fake text has become much more accessible. We hope to highlight the current state of text generation to demonstrate how difficult it is to discern fiction from reality.”

Since then several organizations and researchers have shown that it is [possible to develop systems to detect “fake text”](https://www.theguardian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information). We believe that you too can implement a competitive system.

This assignment is not about Natural Language Processing (NLP) but about being able to deal with sequential data using deep learning. Some basic knowledge from M2 can be useful to squeeze the last 1% performance but you should be able to get great results with pure Keras. The data can be found [here](https://github.com/DeepLearnI/trump_tweet_classifier/raw/master/code/tweet_labels.csv) and has the following format:


There are 8000 real Trump tweet sand 7348 fake ones.

Having been with us for M2, you may ask: How do I go from text to numerical values that I can use in a neural model?
You can for instance just use the “Tokenizer” that’s part of Keras. 

vocabulary_size = 5000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])

This will turn the tweets into sequences of indices. From here, you just need to “pad” the sequences (for en easier workflow). Here, the tweets were padded to a length of 100 with pre-padding (leading 0s are attached to reach the same length for all tweets). But what kind of network do you expect me to build? That’s up to you. Just make sure it is more advanced than a basic feed-forward-net. Not acceptable solution: You use SpaCy or similar to get average vectors for the tweets and then build an M1 style classifier just using a neural model rather than a random forest or similar.

If you want to build such a simple model as a baseline - that’s fine, but we do expect that you come up with something more advanced.

*   You can use CNNs, RNNs (GRU, LSTM, BiLSTMS), Embedding layers in combinations that you think are useful (explain your choices). 
*   If you want, you are welcome to use pre-trained word-vectors (public domain or homemade). 
*   You can go even deeper and explore transformer-based models, attention and the more recent 2018-19 stuff. 
*   BUT: A top-performance can be achieved just using Keras and no fancy NLP stuff. Be creative and use the 100s of tutorials and recipes on the web.


# Assignment

```{r include=FALSE}
library(tidyverse)
library(keras)
library(caret)
library(tidytext)
library(rsample)
library(glmnet)
library(yardstick)

fake <- read_csv("https://github.com/DeepLearnI/trump_tweet_classifier/raw/master/code/tweet_labels.csv")
fake$id <- c(1:nrow(fake))
```


## Setting up baseline
```{r}
set.seed(2)
Index <- createDataPartition(fake$labels, p = 0.70, list = FALSE, times = 1)

faketrain  <- fake[ Index,]
faketest   <- fake[-Index,]
```



```{r}
tidy_fake <- faketrain %>%
  unnest_tokens(word, tweet) %>%
  group_by(word) %>%
  filter(n() > 10) %>%
  ungroup()  %>% 
  anti_join(stop_words, by = "word")

sparse_words <- tidy_fake %>%
  count(id, word) %>%
  cast_sparse(id, word, n)

word_rownames <- as.integer(rownames(sparse_words))

joined <- data_frame(id = word_rownames) %>%
  left_join(fake %>% select(id, labels))

model <- cv.glmnet(sparse_words, joined$labels, family = "binomial", keep = T)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)

intercept <- coefs %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)

classifications <- faketest %>%
  unnest_tokens(word, tweet) %>%
  group_by(word) %>%
  filter(n() > 10) %>%
  ungroup()  %>% 
  anti_join(stop_words, by = "word") %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(id) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(score))

res <- classifications %>% 
  left_join(fake, by = "id")

result <- table(res$probability>0.5, res$labels)
result
accuracy(result)
```




```{r fig.height=4, fig.width=10}
model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se) %>%
  group_by(estimate > 0) %>%
  top_n(20, abs(estimate)) %>% 
  mutate(lab = ifelse(estimate > 0, "Top coeffecients", "Bottom Coeffecients")) %>% 
  ggplot(aes(reorder(term, estimate), estimate)) +
    geom_col(alpha = 0.7, show.legend = FALSE, width = 0.8, fill="black") +
    coord_flip() +
    geom_hline(aes(yintercept = 0)) +
    scale_x_reordered() +
    scale_fill_brewer(palette = "Paired") +
    labs(title = "Words affecting wether the model characterizes the tweet as true", subtitle="Only words that appear more than 10 times", x=NULL, y=NULL) +
    scale_y_continuous(limits = c(-4,4), breaks = seq(-4, 4, by = 1)) + 
  facet_wrap(~lab, scales="free")
```




## Setting up neural networks
```{r}
tokenizer <- text_tokenizer(num_words = 10000)
tokenizer %>% fit_text_tokenizer(faketrain$tweet)
text_seqs <- texts_to_sequences(tokenizer, faketrain$tweet)
x_train <- text_seqs %>% pad_sequences(maxlen = 60) 
y_train <- faketrain %>% select(labels) %>% as.matrix()

tokenizer <- text_tokenizer(num_words = 10000)
tokenizer %>% fit_text_tokenizer(faketest$tweet)
text_seqs <- texts_to_sequences(tokenizer, faketest$tweet)
x_test <- text_seqs %>% pad_sequences(maxlen = 60) 
y_test <- faketest %>% select(labels) %>% as.matrix()
# Naive model -------------------------------------------------------------

model <- keras_model_sequential() %>%
  layer_embedding(10000, 50, input_length = 60) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(filters = 64, kernel_size = 3,padding = "valid", activation = "relu", strides = 1) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(50) %>%
  layer_dropout(0.2) %>%
  layer_activation("relu") %>%
  layer_dense(1) %>%
  layer_activation("sigmoid")

model %>% compile(optimizer = 'adam',
                  loss      = 'binary_crossentropy',
                  metrics   = 'accuracy') 

model %>% fit(x_train, y_train,
    batch_size = 100,
    epochs = 10,
    validation_split = 0.1)


model %>% evaluate(x_test, y_test, verbose = 0)
```



```{r}
tokenizer <- text_tokenizer(num_words = 10000)
tokenizer %>% fit_text_tokenizer(faketrain$tweet)
text_seqs <- texts_to_sequences(tokenizer, faketrain$tweet)
x_train <- text_seqs %>% pad_sequences(maxlen = 60) 
y_train <- faketrain %>% select(labels) %>% as.matrix()

tokenizer <- text_tokenizer(num_words = 10000)
tokenizer %>% fit_text_tokenizer(faketest$tweet)
text_seqs <- texts_to_sequences(tokenizer, faketest$tweet)
x_test <- text_seqs %>% pad_sequences(maxlen = 60) 
y_test <- faketest %>% select(labels) %>% as.matrix()
# Naive model -------------------------------------------------------------

model <- keras_model_sequential() %>%
  layer_embedding(10000, 50, input_length = 60) %>%
  layer_cudnn_lstm(units = 64) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

model %>% compile(optimizer = 'adam',
                  loss      = 'binary_crossentropy',
                  metrics   = 'accuracy') 

model %>% fit(x_train, y_train,
    batch_size = 100,
    epochs = 10,
    validation_split = 0.1)


model %>% evaluate(x_test, y_test, verbose = 0)
```


## Performance comparison




# Details

Please submit a PDF or HTML version of your notebook on peergrade.io (if you submit HTML, please zip it before - large embedded HTMLs from cause crashing when opened directly in peergrade). In addition, include a link to a functioning google Colab notebook or Kaggle kernel (mandatory). Please make sure it runs without errors and others can access it (i.e. own test in “anonymous” setting in your browser).

This notebook should:

*   solve the questions in a straightforward and elegant way.
*   contain enough explanations to enable your fellow students (or others on a similar level of knowledge) to clearly understand what you are doing, why, what is the outcome, how to interpret it, and how to reconstruct the exercise. Be specific and understandable, but brief.

Further process and dates:

*   You will receive an upload link on peergrade.io by 15.11 
*   The notebook upload is due 18/11; 23.50. Delays are not accepted.
*   After the upload deadline, you will receive an invitation to peergrade your fellows' exams on peergrade.io. You will be asked for the evaluation of 3 peer-assignments is part of the assignment and mandatory.
*   The peergrade evaluation is due 21/11. Reviewing is part of your performance. Don’t forget that.

